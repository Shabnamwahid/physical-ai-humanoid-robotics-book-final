"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9328],{7693:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"appendices/code-examples","title":"Code Examples Repository","description":"Complete code listings referenced in the textbook","source":"@site/docs/appendices/code-examples.md","sourceDirName":"appendices","slug":"/appendices/code-examples","permalink":"/docs/appendices/code-examples","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/appendices/code-examples.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Code Examples Repository","sidebar_position":3,"description":"Complete code listings referenced in the textbook"},"sidebar":"tutorialSidebar","previous":{"title":"Installation Guides","permalink":"/docs/appendices/installation-guides"},"next":{"title":"Troubleshooting Guide","permalink":"/docs/appendices/troubleshooting"}}');var t=i(4848),o=i(8453);const a={title:"Code Examples Repository",sidebar_position:3,description:"Complete code listings referenced in the textbook"},r="Appendix B: Code Examples Repository",l={},c=[{value:"Table of Contents",id:"table-of-contents",level:2},{value:"Chapter 3: ROS 2 Examples",id:"chapter-3-ros-2-examples",level:2},{value:"Simple Publisher Node",id:"simple-publisher-node",level:3},{value:"Simple Subscriber Node",id:"simple-subscriber-node",level:3},{value:"Service Server Example",id:"service-server-example",level:3},{value:"Chapter 4: Simulation Examples",id:"chapter-4-simulation-examples",level:2},{value:"Gazebo Model Spawn Script",id:"gazebo-model-spawn-script",level:3},{value:"Unity Simulation Integration (C# Example)",id:"unity-simulation-integration-c-example",level:3},{value:"Chapter 5: Isaac Examples",id:"chapter-5-isaac-examples",level:2},{value:"Isaac ROS Perception Pipeline",id:"isaac-ros-perception-pipeline",level:3},{value:"Chapter 6: VLA Examples",id:"chapter-6-vla-examples",level:2},{value:"Vision-Language-Action Integration",id:"vision-language-action-integration",level:3},{value:"Chapter 7: Conversational AI Examples",id:"chapter-7-conversational-ai-examples",level:2},{value:"Simple Chat Interface with Context",id:"simple-chat-interface-with-context",level:3},{value:"Utility Functions",id:"utility-functions",level:2},{value:"Vector Database Integration",id:"vector-database-integration",level:3},{value:"Configuration Management",id:"configuration-management",level:3},{value:"Running the Examples",id:"running-the-examples",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"appendix-b-code-examples-repository",children:"Appendix B: Code Examples Repository"})}),"\n",(0,t.jsx)(n.p,{children:"This appendix contains complete code examples referenced throughout the textbook, organized by chapter and topic."}),"\n",(0,t.jsx)(n.h2,{id:"table-of-contents",children:"Table of Contents"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#chapter-3-ros-2-examples",children:"Chapter 3: ROS 2 Examples"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#chapter-4-simulation-examples",children:"Chapter 4: Simulation Examples"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#chapter-5-isaac-examples",children:"Chapter 5: Isaac Examples"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#chapter-6-vla-examples",children:"Chapter 6: VLA Examples"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#chapter-7-conversational-ai-examples",children:"Chapter 7: Conversational AI Examples"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"#utility-functions",children:"Utility Functions"})}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"chapter-3-ros-2-examples",children:"Chapter 3: ROS 2 Examples"}),"\n",(0,t.jsx)(n.h3,{id:"simple-publisher-node",children:"Simple Publisher Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\n\nclass MinimalPublisher(Node):\n\n    def __init__(self):\n        super().__init__('minimal_publisher')\n        self.publisher_ = self.create_publisher(String, 'topic', 10)\n        timer_period = 0.5  # seconds\n        self.timer = self.create_timer(timer_period, self.timer_callback)\n        self.i = 0\n\n    def timer_callback(self):\n        msg = String()\n        msg.data = 'Hello World: %d' % self.i\n        self.publisher_.publish(msg)\n        self.get_logger().info('Publishing: \"%s\"' % msg.data)\n        self.i += 1\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    minimal_publisher = MinimalPublisher()\n\n    rclpy.spin(minimal_publisher)\n\n    # Destroy the node explicitly\n    minimal_publisher.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"simple-subscriber-node",children:"Simple Subscriber Node"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\n\nclass MinimalSubscriber(Node):\n\n    def __init__(self):\n        super().__init__('minimal_subscriber')\n        self.subscription = self.create_subscription(\n            String,\n            'topic',\n            self.listener_callback,\n            10)\n        self.subscription  # prevent unused variable warning\n\n    def listener_callback(self, msg):\n        self.get_logger().info('I heard: \"%s\"' % msg.data)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    minimal_subscriber = MinimalSubscriber()\n\n    rclpy.spin(minimal_subscriber)\n\n    # Destroy the node explicitly\n    minimal_subscriber.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"service-server-example",children:"Service Server Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nfrom example_interfaces.srv import AddTwoInts\nimport rclpy\nfrom rclpy.node import Node\n\n\nclass MinimalService(Node):\n\n    def __init__(self):\n        super().__init__('minimal_service')\n        self.srv = self.create_service(AddTwoInts, 'add_two_ints', self.add_two_ints_callback)\n\n    def add_two_ints_callback(self, request, response):\n        response.sum = request.a + request.b\n        self.get_logger().info('Incoming request\\na: %d b: %d' % (request.a, request.b))\n        return response\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    minimal_service = MinimalService()\n\n    rclpy.spin(minimal_service)\n\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"chapter-4-simulation-examples",children:"Chapter 4: Simulation Examples"}),"\n",(0,t.jsx)(n.h3,{id:"gazebo-model-spawn-script",children:"Gazebo Model Spawn Script"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom gazebo_msgs.srv import SpawnEntity\nimport sys\nimport os\n\n\nclass SpawnNode(Node):\n\n    def __init__(self):\n        super().__init__('spawn_node')\n        self.cli = self.create_client(SpawnEntity, '/spawn_entity')\n        while not self.cli.wait_for_service(timeout_sec=1.0):\n            self.get_logger().info('Service not available, waiting again...')\n\n    def send_request(self):\n        # Load the URDF file\n        urdf_path = sys.argv[1] if len(sys.argv) > 1 else 'path/to/robot.urdf'\n\n        with open(urdf_path, 'r') as urdf_file:\n            urdf = urdf_file.read()\n\n        request = SpawnEntity.Request()\n        request.name = \"my_robot\"\n        request.xml = urdf\n        request.initial_pose.position.x = 0.0\n        request.initial_pose.position.y = 0.0\n        request.initial_pose.position.z = 1.0\n        self.future = self.cli.call_async(request)\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    spawn_node = SpawnNode()\n    spawn_node.send_request()\n\n    while rclpy.ok():\n        rclpy.spin_once(spawn_node)\n        if spawn_node.future.done():\n            try:\n                response = spawn_node.future.result()\n            except Exception as e:\n                spawn_node.get_logger().info('Service call failed %r' % (e,))\n            else:\n                spawn_node.get_logger().info('Result %r' % (response.success,))\n            break\n\n    spawn_node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"unity-simulation-integration-c-example",children:"Unity Simulation Integration (C# Example)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using UnityEngine;\nusing System.Collections;\nusing System.Collections.Generic;\nusing Newtonsoft.Json;\n\npublic class RobotController : MonoBehaviour\n{\n    public float moveSpeed = 5.0f;\n    public float rotationSpeed = 100.0f;\n\n    private Dictionary<string, System.Action> commandMap;\n\n    void Start()\n    {\n        commandMap = new Dictionary<string, System.Action>\n        {\n            {"move_forward", MoveForward},\n            {"turn_left", TurnLeft},\n            {"turn_right", TurnRight},\n            {"stop", Stop}\n        };\n    }\n\n    void Update()\n    {\n        // Process commands from external source\n        ProcessCommands();\n    }\n\n    public void ProcessCommands()\n    {\n        // In a real implementation, this would receive commands from ROS or another system\n        // For example, through TCP/IP, WebSocket, or shared memory\n    }\n\n    public void ExecuteCommand(string command)\n    {\n        if (commandMap.ContainsKey(command))\n        {\n            commandMap[command]();\n        }\n    }\n\n    void MoveForward()\n    {\n        transform.Translate(Vector3.forward * moveSpeed * Time.deltaTime);\n    }\n\n    void TurnLeft()\n    {\n        transform.Rotate(Vector3.up, -rotationSpeed * Time.deltaTime);\n    }\n\n    void TurnRight()\n    {\n        transform.Rotate(Vector3.up, rotationSpeed * Time.deltaTime);\n    }\n\n    void Stop()\n    {\n        // Stop any movement\n    }\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"chapter-5-isaac-examples",children:"Chapter 5: Isaac Examples"}),"\n",(0,t.jsx)(n.h3,{id:"isaac-ros-perception-pipeline",children:"Isaac ROS Perception Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\n\nclass IsaacPerceptionNode(Node):\n\n    def __init__(self):\n        super().__init__('isaac_perception_node')\n        self.subscription = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10)\n        self.publisher = self.create_publisher(Image, 'camera/image_processed', 10)\n        self.bridge = CvBridge()\n\n    def image_callback(self, msg):\n        # Convert ROS Image message to OpenCV image\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n        # Perform computer vision processing\n        processed_image = self.process_image(cv_image)\n\n        # Convert back to ROS Image message\n        processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding='bgr8')\n        processed_msg.header = msg.header\n\n        # Publish processed image\n        self.publisher.publish(processed_msg)\n\n    def process_image(self, image):\n        # Example: Simple edge detection\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n        # Convert back to 3-channel for visualization\n        edge_image = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n        return edge_image\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = IsaacPerceptionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"chapter-6-vla-examples",children:"Chapter 6: VLA Examples"}),"\n",(0,t.jsx)(n.h3,{id:"vision-language-action-integration",children:"Vision-Language-Action Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import torch\nimport clip\nfrom PIL import Image\nimport numpy as np\nimport openai\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\n\nclass VisionLanguageActionSystem:\n    def __init__(self):\n        # Load pre-trained models\n        self.device = "cuda" if torch.cuda.is_available() else "cpu"\n        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)\n\n        # BLIP for image captioning\n        self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")\n        self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(self.device)\n\n        # Initialize action planning system\n        self.action_space = {\n            \'move_forward\': lambda: print("Moving forward"),\n            \'turn_left\': lambda: print("Turning left"),\n            \'turn_right\': lambda: print("Turning right"),\n            \'pick_up\': lambda: print("Picking up object"),\n            \'place_down\': lambda: print("Placing down object")\n        }\n\n    def perceive_scene(self, image_path):\n        """Generate scene description from image"""\n        raw_image = Image.open(image_path).convert(\'RGB\')\n\n        # Generate caption\n        inputs = self.blip_processor(raw_image, return_tensors="pt").to(self.device)\n        out = self.blip_model.generate(**inputs)\n        caption = self.blip_processor.decode(out[0], skip_special_tokens=True)\n\n        return caption\n\n    def understand_command(self, command, scene_description):\n        """Process natural language command in context of scene"""\n        prompt = f"Given the scene: \'{scene_description}\', interpret the command: \'{command}\'. What should be done?"\n\n        # In a real implementation, this would use a more sophisticated model\n        # For now, we\'ll use a simple keyword-based approach\n        if \'move\' in command.lower() and \'forward\' in command.lower():\n            return \'move_forward\'\n        elif \'turn\' in command.lower() and \'left\' in command.lower():\n            return \'turn_left\'\n        elif \'turn\' in command.lower() and \'right\' in command.lower():\n            return \'turn_right\'\n        elif \'pick\' in command.lower() or \'grasp\' in command.lower():\n            return \'pick_up\'\n        elif \'place\' in command.lower() or \'put\' in command.lower():\n            return \'place_down\'\n        else:\n            return None\n\n    def plan_action(self, command, scene_description):\n        """Plan action based on command and scene"""\n        action = self.understand_command(command, scene_description)\n        return action\n\n    def execute_action(self, action):\n        """Execute planned action"""\n        if action in self.action_space:\n            self.action_space[action]()\n            return True\n        else:\n            print(f"Action \'{action}\' not recognized")\n            return False\n\n    def process_interaction(self, image_path, command):\n        """Complete VLA pipeline: Vision -> Language -> Action"""\n        # 1. Perceive scene\n        scene_description = self.perceive_scene(image_path)\n        print(f"Scene: {scene_description}")\n\n        # 2. Understand command in context\n        action = self.plan_action(command, scene_description)\n\n        # 3. Execute action\n        if action:\n            print(f"Executing action: {action}")\n            self.execute_action(action)\n        else:\n            print("Could not determine appropriate action")\n\n\n# Example usage\nif __name__ == "__main__":\n    vla_system = VisionLanguageActionSystem()\n\n    # Example: Process an image with a command\n    # vla_system.process_interaction("path/to/image.jpg", "Move the red block to the left")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"chapter-7-conversational-ai-examples",children:"Chapter 7: Conversational AI Examples"}),"\n",(0,t.jsx)(n.h3,{id:"simple-chat-interface-with-context",children:"Simple Chat Interface with Context"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import openai\nimport json\nfrom datetime import datetime\n\n\nclass ConversationalRobot:\n    def __init__(self, api_key=None):\n        if api_key:\n            openai.api_key = api_key\n\n        # Conversation history\n        self.conversation_history = []\n\n        # System context\n        self.system_context = {\n            "role": "system",\n            "content": "You are a helpful robotic assistant. You can answer questions about robotics, AI, and provide assistance with technical problems. Keep responses concise but informative."\n        }\n\n    def add_to_history(self, role, content):\n        """Add message to conversation history"""\n        message = {\n            "role": role,\n            "content": content,\n            "timestamp": datetime.now().isoformat()\n        }\n        self.conversation_history.append(message)\n\n    def get_response(self, user_input):\n        """Get response from AI model"""\n        # Add user message to history\n        self.add_to_history("user", user_input)\n\n        # Prepare messages for API call\n        messages = [self.system_context] + [\n            {"role": msg["role"], "content": msg["content"]}\n            for msg in self.conversation_history[-10:]  # Use last 10 messages\n        ]\n\n        try:\n            response = openai.ChatCompletion.create(\n                model="gpt-3.5-turbo",\n                messages=messages,\n                max_tokens=200,\n                temperature=0.7\n            )\n\n            ai_response = response.choices[0].message[\'content\'].strip()\n\n            # Add AI response to history\n            self.add_to_history("assistant", ai_response)\n\n            return ai_response\n\n        except Exception as e:\n            error_msg = f"Error getting response: {str(e)}"\n            self.add_to_history("assistant", error_msg)\n            return error_msg\n\n    def reset_conversation(self):\n        """Reset conversation history"""\n        self.conversation_history = []\n\n    def get_conversation_context(self):\n        """Get current conversation context"""\n        return self.conversation_history\n\n\n# Example usage\nif __name__ == "__main__":\n    # Initialize the conversational robot\n    robot = ConversationalRobot(api_key="your-api-key-here")\n\n    # Example conversation\n    response = robot.get_response("Hello, can you help me with ROS 2?")\n    print(f"Robot: {response}")\n\n    response = robot.get_response("How do I create a publisher node?")\n    print(f"Robot: {response}")\n'})}),"\n",(0,t.jsx)(n.h2,{id:"utility-functions",children:"Utility Functions"}),"\n",(0,t.jsx)(n.h3,{id:"vector-database-integration",children:"Vector Database Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\n\nclass VectorDB:\n    def __init__(self, host="localhost", port=6333):\n        self.client = QdrantClient(host=host, port=port)\n        self.model = SentenceTransformer(\'all-MiniLM-L6-v2\')\n\n        # Create collection if it doesn\'t exist\n        try:\n            self.client.get_collection("textbook_content")\n        except:\n            self.client.create_collection(\n                collection_name="textbook_content",\n                vectors_config=models.VectorParams(size=384, distance=models.Distance.COSINE),\n            )\n\n    def add_document(self, text, metadata=None):\n        """Add a document to the vector database"""\n        embedding = self.model.encode([text])[0].tolist()\n\n        self.client.upsert(\n            collection_name="textbook_content",\n            points=[\n                models.PointStruct(\n                    id=len(self.client.scroll(collection_name="textbook_content")[0]),\n                    vector=embedding,\n                    payload={"text": text, **(metadata or {})}\n                )\n            ]\n        )\n\n    def search(self, query, limit=5):\n        """Search for similar documents"""\n        query_embedding = self.model.encode([query])[0].tolist()\n\n        results = self.client.search(\n            collection_name="textbook_content",\n            query_vector=query_embedding,\n            limit=limit\n        )\n\n        return [(hit.payload["text"], hit.score) for hit in results]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import os\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n\n@dataclass\nclass Config:\n    # Database settings\n    qdrant_url: str = os.getenv("QDRANT_URL", "http://localhost:6333")\n    neon_database_url: str = os.getenv("NEON_DATABASE_URL", "")\n\n    # AI settings\n    openai_api_key: Optional[str] = os.getenv("OPENAI_API_KEY")\n    embedding_model: str = os.getenv("EMBEDDING_MODEL", "all-MiniLM-L6-v2")\n\n    # Server settings\n    host: str = os.getenv("HOST", "0.0.0.0")\n    port: int = int(os.getenv("PORT", "8000"))\n\n    # Security\n    debug: bool = os.getenv("DEBUG", "False").lower() == "true"\n\n    # Rate limiting\n    max_tokens: int = int(os.getenv("MAX_TOKENS", "1000"))\n    temperature: float = float(os.getenv("TEMPERATURE", "0.7"))\n\n\n# Usage\nconfig = Config()\n'})}),"\n",(0,t.jsx)(n.h2,{id:"running-the-examples",children:"Running the Examples"}),"\n",(0,t.jsx)(n.p,{children:"To run these examples:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Ensure you have the required dependencies installed:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip install rclpy  # For ROS 2 examples\npip install opencv-python  # For computer vision\npip install torch torchvision torchaudio  # For deep learning\npip install openai  # For API access\npip install transformers  # For NLP\npip install qdrant-client  # For vector database\npip install sentence-transformers  # For embeddings\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"For ROS 2 examples, make sure to source your ROS 2 environment:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"source /opt/ros/humble/setup.bash\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Run Python examples directly:"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"python3 example_file.py\n"})}),"\n",(0,t.jsx)(n.p,{children:"These examples provide a foundation for the concepts discussed in the textbook. You can extend them based on your specific requirements and use cases."})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>r});var s=i(6540);const t={},o=s.createContext(t);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);